<!DOCTYPE html>
<html>
<body>

<h3> 1. Miscellaneous Topics</h3>
<!2nd ordered list>
<h4> 1.1 Attention Mechanism</h4>
<ol>
   <li>
    [Attention 2017 NIPS]
    <a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf"> Attention Is All You Need </a>  
    <a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch"> [PyTorch, </a> 
    <a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py"> TensorFlow, </a>
    <a href="https://lilianweng.github.io/lil-log/archive.html"> Blog] </a> 
  </li> 
</ol>

<h4> 1.2 Few-Shot Learning </h4>
<ol>
   <li>
    [Paper With Code 2019]
    <a href="https://paperswithcode.com/task/few-shot-learning"> Paper With Code (FSL) 2019 </a>
  </li> 
</ol>
   
<h4> 1.3 ICCV 2019 </h4>
<ol>
   <li>
    [SinGAN 2019 iccv - Best paper]
    <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Shaham_SinGAN_Learning_a_Generative_Model_From_a_Single_Natural_Image_ICCV_2019_paper.pdf"> SinGAN: Learning a Generative Model from a Single Natural Image </a>
    <a href="https://github.com/tamarott/SinGAN"> [PyTorch] </a> 
   </li> 
   
   <li>
    <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Gupta_Asynchronous_Single-Photon_3D_Imaging_ICCV_2019_paper.pdf"> Asynchronous Single-Photon 3D Imaging </a>
   </li> 
   
   <li>
    <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Ashual_Specifying_Object_Attributes_and_Relations_in_Interactive_Scene_Generation_ICCV_2019_paper.pdf"> Specifying Object Attributes and Relations in Interactive Scene Generation </a>
   </li> 
</ol>

<h4> 1.4 Pose Estimation </h4>

   <ol>
      <li> [openpose 2018 arxiv]
         <a href="https://arxiv.org/pdf/1907.05686.pdf"> OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields </a>  
         <a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose"> [CMU, </a> 
         <a href="https://github.com/ildoonet/tf-pose-estimation"> OpenPose-Tensorflow-MobileNetV2] </a>
  
      </li>
  </ol>  
     
<h4> 1.5 Network Compression </h4>

   <ol>
      <li> [kill the bits 2019 arxiv]
         <a href="https://arxiv.org/pdf/1812.08008.pdf"> And the bit goes down: Revisiting the quantization of neural networks </a>  
         <a href="https://github.com/facebookresearch/kill-the-bits"> [Pytorch] </a>
  
      </li>
  </ol>  
     

<!########################################################2nd ordered list>
<h3> 2. Deep Learning Resources</h3>

<!3rd ordered list>
<h4> 2.1 Convolutional Neural Network Architectures</h4>

<ol>

   <li>
    [efficientnet 2019 arxiv]
    <a href="https://arxiv.org/pdf/1905.11946.pdf"> EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks </a>  
    <a href="https://github.com/lukemelas/EfficientNet-PyTorch"> [PyTorch, </a> 
    <a href="https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet"> TensorFlow] </a>
  </li> 
 
   <li>
    [nasnet 2018 cvpr]
    <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zoph_Learning_Transferable_Architectures_CVPR_2018_paper.pdf"> Learning Transferable Architectures for Scalable Image Recognition </a>  
    <a href="https://github.com/pytorch/vision/blob/master/torchvision/models/mnasnet.py"> [PyTorch, </a> 
    <a href="https://github.com/tensorflow/models/tree/master/research/slim/nets/nasnet"> TensorFlow] </a>
  </li>
  
  <li>
    [senet 2018 cvpr]
    <a href="https://arxiv.org/pdf/1709.01507.pdf"> Squeeze-and-Excitation Networks </a> 
    <a href="https://github.com/moskomule/senet.pytorch"> [PyTorch, </a> 
    <a href="https://github.com/taki0112/SENet-Tensorflow"> TensorFlow, </a> 
    <a href="https://github.com/hujie-frank/SENet.git"> Caffe] </a>
  </li> 
  
  <li>
    [shufflenet-v2 2018 cvpr]
    <a href="https://arxiv.org/pdf/1807.11164.pdf"> ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design </a> 
    <a href="https://github.com/pytorch/vision/blob/master/torchvision/models/shufflenetv2.py"> [PyTorch, </a> 
    <a href="https://github.com/tensorpack/tensorpack/tree/master/examples/ImageNetModels#shufflenet"> TensorFlow] </a>
  </li> 
    
  <li>
    [shufflenet-v1 2018 cvpr]
    <a href="https://arxiv.org/pdf/1707.01083.pdf"> ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices </a> 
    <a href="https://github.com/jaxony/ShuffleNet"> [PyTorch, </a> 
    <a href="https://github.com/MG2033/ShuffleNet"> TensorFlow] </a>
  </li> 
 
  <li>
    [mobilenet-v3 2019 arxiv]
    <a href="https://arxiv.org/abs/1905.02244"> Searching for MobileNetV3 </a>  
    <a href="https://github.com/kuan-wang/pytorch-mobilenet-v3"> [PyTorch, </a> 
    <a href="https://ai.googleblog.com/2019/11/introducing-next-generation-on-device.html/"> Blog, </a>
    <a href="https://pypi.org/project/mobilenet-v3/"> TensorFlow] </a>
  </li> 
   
  <li>
    [mobilenet-v2 2018 cvpr]
    <a href="https://arxiv.org/pdf/1801.04381.pdf"> MobileNetV2: Inverted Residuals and Linear Bottlenecks </a>  
    <a href="https://github.com/pytorch/vision/blob/master/torchvision/models/mobilenet.py"> [PyTorch, </a> 
    <a href="https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet"> TensorFlow] </a>
  </li> 
  
  <li>
    [mobilenet-v1 2017 arxiv]
    <a href="https://arxiv.org/pdf/1704.04861.pdf"> MobileNets: Efficient Convolutional Neural Networks for Mobile Vision </a>  
    <a href="https://github.com/marvis/pytorch-mobilenet"> [PyTorch, </a> 
    <a href="https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md">  TensorFlow] </a>
  </li>
   
  <li>
    [deformable-covnet 2017 iccv]
    <a href="https://arxiv.org/pdf/1703.06211.pdf"> Deformable Convolutional Networks </a>  
    <a href="https://github.com/open-mmlab/mmdetection"> [PyTorch, </a> 
    <a href="https://github.com/msracver/Deformable-ConvNets"> MxNet] </a>
  </li> 
     
  <li>
    [squeezenet 2017 iclr]
    <a href="https://arxiv.org/pdf/1602.07360.pdf"> SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size </a>  
    <a href="https://github.com/pytorch/vision/blob/master/torchvision/models/squeezenet.py"> [PyTorch, </a> 
    <a href="https://github.com/tensorflow/tpu/tree/master/models/official/squeezenet"> TensorFlow] </a>
  </li> 
    
  <li>
    [densenet 2017 cvpr]
    <a href="https://arxiv.org/pdf/1608.06993.pdf"> DenseNet: Densely Connected Convolutional Networks </a>  
    <a href="https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py"> [PyTorch, </a> 
    <a href="https://github.com/tensorflow/tpu/tree/master/models/official/densenet">  TensorFlow] </a>
  </li> 
  
  <li>
    [resnext 2017 cvpr]
    <a href="https://arxiv.org/pdf/1611.05431.pdf"> Aggregated Residual Transformations for Deep Neural Networks </a>  
    <a href="https://pytorch.org/hub/pytorch_vision_resnext/"> [PyTorch, </a> 
    <a href="https://github.com/tensorpack/tensorpack/tree/master/examples/ResNet#imagenet-resnetpy"> TensorFlow] </a>
  </li> 
  
  <li>
    [wide resnet 2017 arxiv]
    <a href="https://arxiv.org/pdf/1605.07146.pdf"> Wide Residual Networks </a>  
    <a href="https://github.com/szagoruyko/wide-residual-networks/tree/master/pytorch"> [PyTorch, </a> 
    <a href="https://github.com/szagoruyko/wide-residual-networks"> TensorFlow] </a>
  </li> 

  <li>
    [resnet 2016 cvpr]
    <a href="https://arxiv.org/pdf/1512.03385.pdf"> Deep Residual Learning for Image Recognition </a>  
    <a href="https://github.com/szagoruyko/wide-residual-networks"> [PyTorch, </a> 
    <a href="https://github.com/tensorflow/models/tree/master/official/r1/resnet"> TensorFlow] </a>
  </li> 
   
  <li>
    [dialatednet 2016 iclr]
    <a href="https://arxiv.org/pdf/1511.07122.pdf"> Yu, Fisher, and Vladlen Koltun. "Multi-scale context aggregation by dilated convolutions </a>  
    <a href="https://github.com/fyu/dilation"> [Caffe, </a> 
    <a href="https://github.com/torch/nn/blob/master/doc/convolution.md#nn.SpatialDilatedConvolution"> Pytorch] </a>
  </li> 

   <li>
    [vggnet 2015 iclr]
    <a href="https://arxiv.org/pdf/1409.1556.pdf"> Very Deep Convolutional Networks For Large Scale Image Recognition </a>  
    <a href="https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py"> [PyTorch, </a> 
    <a href="https://github.com/tensorflow/models/blob/master/research/slim/nets/vgg.py"> TensorFlow] </a>
  </li> 

  <li>
    [inceptionnet 2015 cvpr]
    <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43022.pdf"> Going Deeper with Convolutions </a>  
    <a href="https://github.com/pytorch/vision/blob/master/torchvision/models/inception.py"> [PyTorch, </a> 
    <a href="https://github.com/tensorflow/models/tree/master/research/inception"> TensorFlow] </a>
  </li> 
  
   <li>
    [alexnet 2012 acm]
    <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"> ImageNet Classification with Deep Convolutional Neural Networks </a>  
    <a href="https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py"> [PyTorch, </a> 
    <a href="https://github.com/tensorflow/models/blob/master/research/slim/nets/alexnet.py"> TensorFlow] </a>
  </li> 
  
</ol> 
 
<!4th Object Detection>
<h4> 2.2 Object Detection</h4>
<ol> 
  
  <li>
    [sota 2019 coco]
    <a href="https://paperswithcode.com/sota/object-detection-on-coco"> sota on COCO dataset </a> 
    <a href="https://github.com/amusi/awesome-object-detection"> awesome-object-detection </a>  
  </li> 
   
  <li>
    [moco 2019-11 arxiv]
    <a href="https://arxiv.org/pdf/1911.05722.pdf"> Momentum Contrast for Unsupervised Visual Representation Learning </a>  
  </li> 
   
  <li>
    [mmdetection 2019 arxiv]
    <a href="https://arxiv.org/pdf/1906.07155.pdf"> MMDetection: Open MMLab Detection Toolbox and Benchmark </a> 
    <a href="https://github.com/open-mmlab/mmdetection"> [PyTorch] </a>  
  </li> 
  
  
  <li>
    [tridentnet 2019 iccv]
    <a href="https://arxiv.org/pdf/1901.01892v2.pdf"> Scale-Aware Trident Networks for Object Detection</a>  
    <a href="https://git.io/fj5vR"> [MXNet, </a> 
    <a href="https://zhuanlan.zhihu.com/p/54334986"> Blog, </a> 
    <a href="https://github.com/facebookresearch/detectron2/tree/master/projects/TridentNet"> PyTorch] </a>
  </li> 
  
  <li>
    [retinanet 2017 iccv]
    <a href="https://arxiv.org/pdf/1708.02002.pdf"> Focal Loss for Dense Object Detection </a>  
    <a href="https://github.com/facebookresearch/Detectron"> [PyTorch, </a> 
    <a href="https://github.com/tensorflow/tpu/tree/master/models/official/retinanet"> TensorFlow] </a>
  </li> 

  <li>
    [mask-rcnn 2017 cvpr]
    <a href="https://arxiv.org/pdf/1703.06870.pdf"> Mask-RCNN </a>  
    <a href="https://github.com/facebookresearch/maskrcnn-benchmark"> [PyTorch, </a> 
    <a href="https://github.com/matterport/Mask_RCNN"> TensorFlow] </a>
  </li> 
   
  <li>
    [faster-rcnn 2017 nips]
    <a href="https://arxiv.org/pdf/1506.01497.pdf"> Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks </a>  
    <a href="https://github.com/pytorch/vision/blob/master/torchvision/models/detection/faster_rcnn.py"> [PyTorch, </a> 
    <a href="https://github.com/tensorflow/models/tree/master/research/object_detection/models"> TensorFlow] </a>
  </li> 
  
  <li>
    [fast-rcnn 2015 iccv]
    <a href="https://arxiv.org/pdf/1504.08083"> Fast R-CNN </a>  
    <a href="https://github.com/facebookresearch/Detectron/blob/master/detectron/modeling/fast_rcnn_heads.py"> [PyTorch, </a> 
    <a href="https://github.com/zplizzi/tensorflow-fast-rcnn"> TensorFlow] </a>
  </li> 
  
  <li>
    [yolo-v3 2018 arxiv]
    <a href="https://arxiv.org/pdf/1506.02640.pdf"> You Only Look Once: Unified, Real-Time Object Detection</a>  
    <a href="https://github.com/eriklindernoren/PyTorch-YOLOv3"> [PyTorch, </a> 
    <a href="https://github.com/ultralytics/yolov3"> PyTorch-ios, </a> 
    <a href="https://github.com/YunYang1994/tensorflow-yolov3"> TensorFlow] </a>
  </li> 
 
  <li>
    [yolo-v2 2017 cvpr]
    <a href="https://arxiv.org/pdf/1506.02640.pdf"> YOLO9000: Better, Faster, Stronger</a>  
    <a href="https://github.com/marvis/pytorch-yolo2"> [PyTorch, </a> 
    <a href="https://github.com/thtrieu/darkflow"> TensorFlow] </a>
  </li> 
  
  <li>
    [yolo-v1 2015 cvpr]
    <a href="https://arxiv.org/pdf/1506.02640.pdf"> You Only Look Once: Unified, Real-Time Object Detection</a>  
    <a href="https://github.com/xiongzihua/pytorch-YOLO-v1"> [PyTorch, </a> 
    <a href="https://github.com/thtrieu/darkflow"> TensorFlow] </a>
  </li>
  
  <li>
    [fpn (feature-pyramid-networks) 2017 cvpr]
    <a href="https://arxiv.org/pdf/1612.03144"> Feature Pyramid Networks for Object Detection</a>  
    <a href="https://github.com/funnyzhou/FPN-Pytorch"> [PyTorch, </a> 
    <a href="https://github.com/DetectionTeamUCAS/FPN_Tensorflow"> TensorFlow] </a>
  </li> 
  
  <li>
    [ssd 2016 eccv]
    <a href="https://arxiv.org/pdf/1512.02325"> SSD: Single shot multibox detector</a>  
    <a href="https://pytorch.org/hub/nvidia_deeplearningexamples_ssd/"> [PyTorch, </a> 
    <a href="https://github.com/balancap/SSD-Tensorflow"> TensorFlow, </a>
    <a href="https://github.com/weiliu89/caffe/tree/ssd"> Caffe] </a>
  </li> 
  
  <li>
    [u-net 2015 miccai]
    <a href="https://arxiv.org/pdf/1505.04597"> U-Net: Convolutional Networks for Biomedical Image Segmentation </a> 
    <a href="https://github.com/milesial/Pytorch-UNet/"> [PyTorch, </a> 
    <a href="https://www.kaggle.com/vijaybj/basic-u-net-using-tensorflow"> TensorFlow] </a>
  </li>  
  
  <li>
    [fcn (fully-convolutional-networks) 2015 cvpr]
    <a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf"> Fully Convolutional Networks for Semantic Segmentation </a> 
    <a href="https://github.com/wkentaro/pytorch-fcn"> [PyTorch, </a> 
    <a href="https://github.com/shekkizh/FCN.tensorflow"> TensorFlow] </a>
  </li>
  
   
  <li>
    [deepLab-v3+ 2018 eccv]
    <a href="https://arxiv.org/abs/1802.02611"> Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation </a> 
    <a href="https://github.com/kazuto1011/deeplab-pytorch"> [Pytorch, </a> 
    <a href="https://github.com/tensorflow/models/tree/master/research/deeplab"> TensorFlow] </a>
  </li>
  
  <li>
    [deepLab-v3 2018 pami]
    <a href="http://arxiv.org/abs/1706.05587"> Rethinking Atrous Convolution for Semantic Image Segmentation </a> 
    <a href="https://https://github.com/chenxi116/DeepLabv3.pytorch"> [Pytorch, </a> 
    <a href="https://github.com/tensorflow/models/tree/master/research/deeplab"> TensorFlow] </a>
  </li>
    
   <li>
    [deepLab-v2 2018 pami]
    <a href="https://arxiv.org/abs/1606.00915"> DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs </a> 
    <a href="https://github.com/kazuto1011/deeplab-pytorch"> [Pytorch, </a> 
    <a href="https://github.com/tensorflow/models/tree/master/research/deeplab"> TensorFlow, </a>
    <a href="https://bitbucket.org/deeplab/deeplab-public/src/master/"> Caffe] </a> 

  </li>

  <li>
    [deepLab-v1 2015 iclr]
    <a href="https://arxiv.org/abs/1412.7062"> Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs </a> 
    <a href="https://github.com/kazuto1011/deeplab-pytorch"> [PyTorch, </a> 
    <a href="https://github.com/tensorflow/models/tree/master/research/deeplab"> TensorFlow] </a>
  </li>
  
</ol>

<!5th ordered list>
<h4> 2.3 Generative Adversarial Nets </h4>
<ol> 
 
  <li>
    [big-gan 2019 iclr]
    <a href="https://arxiv.org/pdf/1809.11096.pdf"> Large Scale GAN Training for High Fidelity Natural Image Synthesis</a> 
    <a href="https://github.com/ajbrock/BigGAN-PyTorch"> [Pytorch] </a>
  </li>
   
 <li>
    [circle-gan 2017 iccv]
    <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.pdf"> Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks </a> 
    <a href="https://github.com/junyanz/CycleGAN"> [Tensorflow, </a>
    <a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"> Pytorch, </a>
    <a href="https://junyanz.github.io/CycleGAN/"> Blog] </a>
  </li>
   
  <li>
    [gan 2014 nips]
    <a href="https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf"> Generative Adversarial Nets </a> 
    <a href="https://github.com/goodfeli/adversarial"> [Theano] </a>
  </li>
   
   
   
</ol>  
   
<!6th ordered list>
<h4> 2.4 Face Recognition</h4>
<ol> 
 
  <li> [dsfd 2019 cvpr] 
  <a href="https://arxiv.org/pdf/1810.10220.pdf"> DSFD: Dual Shot Face Detector </a> 
  <a href="https://github.com/TencentYoutuResearch/FaceDetection-DSFD"> [Pytorch] </a>
  </li> 
   
  <li> [fab 2018 ijcai] 
  <a href="hhttps://www.ijcai.org/proceedings/2018/0102.pdf"> Harnessing Synthesized Abstraction Images to Improve Facial Attribute Recognition </a> 
  <a href="https://github.com/TencentYoutuResearch/FaceAttribute-FAN"> [Caffe] </a>
  </li> 

  <li> [mtcnn 2016 ipl] 
  <a href="https://kpzhang93.github.io/MTCNN_face_detection_alignment/index.html"> Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks </a> 
  <a href="https://kpzhang93.github.io/MTCNN_face_detection_alignment/index.html"> [Github Page,  </a>
  <a href="https://github.com/davidsandberg/facenet/tree/master/src/align"> TensorFlow, </a>
  <a href="https://pypi.org/project/mtcnn/"> pip, </a>
  <a href="https://github.com/kpzhang93/MTCNN_face_detection_alignment"> Matlab/Caffe] </a>
  </li> 
   

 
   <li> [openface 2016 cmu inception-resnet-v1] 
  <a href="http://reports-archive.adm.cs.cmu.edu/anon/anon/usr0/ftp/2016/CMU-CS-16-118.pdf"> Openface: A general-purpose face recognition library with mobile applications </a> 
  </li> 
   
  <li> [facenet 2015 cvpr google zfnet-based] 
  <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf"> Facenet: A unified embedding for face recognition and clustering </a> 
  <a href="https://github.com/davidsandberg/facenet"> [TensorFlow, </a>
  <a href="https://github.com/timesler/facenet-pytorch"> PyTorch] </a>
  </li> 
    
  <li> [deepface 2014 cvpr facebook vgg-like] 
  <a href="https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf"> DeepFace: Closing the Gap to Human-Level Performance in Face Verification </a> 
  </li> 
   
</ol>
  
  
<!6th ordered list>
<h4> 2.5 Regulizer</h4>
<ol>
   
  <li>
    [switchable norm 2019 iclr]
    <a href="https://arxiv.org/pdf/1806.10779.pdf"> Differentiable Learning-to-Normalize via Switchable Normalization </a> 
    <a href="https://github.com/switchablenorms/Switchable-Normalization"> [PyTorch] </a>
  </li>
   
   
  <li>
    [scale norm 2019 arxiv]
    <a href="https://arxiv.org/pdf/1910.05895.pdf"> Transformers without Tears: Improving the Normalization of Self-Attention </a> 
    <a href="https://github.com/hyeonseobnam/Batch-Instance-Normalization"> [PyTorch, </a> 
    <a href="https://pythonawesome.com/simple-tensorflow-implementation-of-batch-instance-normalization/"> TensorFlow] </a>
  </li>

  <li>
    [spectrum norm 2018 iclr]
    <a href="https://arxiv.org/pdf/1802.05957.pdf"> Spectral Normalization for Generative Adversarial Networks </a> 
    <a href="https://github.com/pfnet-research/sngan_projection"> [Tensorflow] </a>
  </li>
   
  <li>
    [batch-instance norm 2018 nips]
    <a href="https://arxiv.org/pdf/1805.07925.pdf"> Do Normalization Layers in a Deep ConvNet Really Need to Be Distinct? </a> 
    <a href="https://github.com/hyeonseobnam/Batch-Instance-Normalization"> [PyTorch, </a> 
    <a href="https://pythonawesome.com/simple-tensorflow-implementation-of-batch-instance-normalization/"> TensorFlow] </a>
  </li>

  <li>
    [group norm 2018 eccv]
    <a href="https://arxiv.org/pdf/1803.08494.pdf"> Group Normalization </a> 
    <a href="https://github.com/facebookresearch/Detectron/tree/master/projects/GN"> [PyTorch, </a> 
    <a href="https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization"> TensorFlow] </a>
  </li>
   
  <li>
    [instance norm 2016 arxiv]
    <a href="https://arxiv.org/pdf/1607.08022.pdf"> Instance Normalization: The Missing Ingredient for Fast Stylization </a> 
    <a href="https://pytorch.org/docs/stable/_modules/torch/nn/modules/instancenorm.html"> [PyTorch, </a> 
    <a href="https://github.com/tensorflow/addons/blob/master/tensorflow_addons/layers/normalizations.py#L277"> TensorFlow] </a>
  </li>
    

  <li>
    [layer norm 2016 arxiv]
    <a href="https://arxiv.org/pdf/1607.06450.pdf"> Layer Normalization </a> 
    <a href="https://pytorch.org/docs/stable/_modules/torch/nn/modules/normalization.html"> [PyTorch, </a> 
    <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization"> TensorFlow, </a>
    <a href="https://mlexplained.com/2018/11/30/an-overview-of-normalization-methods-in-deep-learning/"> Blog] </a>
  </li>
    
    
  <li>
    [batch norm 2015 arxiv]
    <a href="https://arxiv.org/pdf/1502.03167.pdf"> Batch normalization: Accelerating deep network training by reducing internal covariate shift </a> 
    <a href="https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html"> [PyTorch, </a> 
    <a href="https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization"> TensorFlow] </a>
  </li>
    
  <li>
    [dropout 2014 jmlr]
    <a href="http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf"> Dropout: A Simple Way to Prevent Neural Networks from Overfitting </a> 
    <a href="https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/dropout.py"> [PyTorch, </a> 
    <a href="https://www.tensorflow.org/api_docs/python/tf/nn/dropout"> TensorFlow] </a>
  </li>
  
</ol>
  
<!7th ordered list>
<h4> 2.6 Datasets & Tools</h4> 
   
<h5> 2.5.1 CV Datasets</h5> 
<ol> 
  <li> <a href="http://cocodataset.org/#download"> COCO Dataset,  </a>  
       <a href="https://github.com/eriklindernoren/PyTorch-YOLOv3/blob/master/data/coco.names"> COCO Index & Names </a></li> 
  <li><a href="http://www.robots.ox.ac.uk/~vgg/data/vgg_face/"> VGGFace : 2622 identities </a></li> 
  <li><a href="http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/"> VGGFace2: 9k identities, 3.3 million images </a></li> 
  <li><a href="http://vintage.winklerbros.net/facescrub.html"> FaceScrub : 530 identities, 100K images</a></li> 
  <li><a href="https://fei.edu.br/~cet/facedatabase.html"> FEI Face Database : 2000 identities, 2800 images</a></li> 
  <li><a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/"> VoxCeleb : 7k identities, 1 million utterances, 2k hours</a></li> 
  <li><a href="https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/"> IMDB-WIKI – 500k+ face images with age and gender labels </a></li> 
  <li><a href="http://vis-www.cs.umass.edu/fddb/results.html/"> FDDB - face detection dataset and benchmark </a></li> 
</ol>
<h5> 2.5.2 Image/Video annotators</h5> 
<ol> 
  <li> <a href="https://github.com/tzutalin/labelImg"> LabelIMG Image Annotator (Bounding Box),  </a>  </li> 
  <li> <a href="https://github.com/wkentaro/labelme"> Labelme Image Annotator (BBox, Mask, Line) </a></li> 
  <li> <a href="https://github.com/wkentaro/labelme/blob/master/examples/instance_segmentation/README.md">  Labelme2coco: Prepare coco-like dataset </a></li> 
  <li> <a href="http://www.robots.ox.ac.uk/~vgg/software/via/">  VGG Image/Video Annotator (VIA) </a></li> 
  <li> <a href="https://github.com/tryolabs/luminoth/">  luminoth </a></li> 

</ol>
   
<!8th ordered list>
<h4> 2.7 Explainable AI</h4>

<ol> 
  <li> <a href="https://beenkim.github.io/slides/DLSS2018Vector_Been.pdf"> Introduction to Interpretable Machine Learning </a> </li>  
  <li> <a href=""> Understanding Black-box Predictions via Influence Functions </a> </li>  
  
  <li> TCAV 2018 icml
  <a href="https://arxiv.org/abs/1711.11279.pdf"> Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV) </a> 
  <a href="https://github.com/tensorflow/tcav"> TensorFlow </a> 
  <a href="https://github.com/soumyadip1995/TCAV"> PyTorch </a> 
  
  </li>
  
</ol>

  
<!9th ordered list>
<h4> 2.8 Blog</h4>

<ol> 
  <li> <a href="https://zhuanlan.zhihu.com/p/76294920"> 基于Movidius的目标检测算法开发（YOLOv3+MobileNet）</a> </li>  
</ol>

<!10th ordered list>
<h4> 2.9 Companies </h4>
<ol> 
  <li> <a href="https://www.4dface.io"> 4Dface.io </a> 4dface: Real-time 3D face tracking and reconstruction from 2D video
  <a href="https://github.com/patrikhuber/4dface"> Website </a>
  </li>  
</ol>
 

<!10th ordered list>
<h4> 2.10 Influencers </h4>
<ol> 
  <li> <a href="http://kaiminghe.com/"> Kaiming He (FAIR)  </a> </li>  
</ol>


<!########################################################1st ordered list>
<h3> 3. Technical Issues</h3>
<!1st ordered list>
<ol>

  <li>keras: load_weights_from_hdf5_group_by_name</li>
  <p> href="https://github.com/matterport/Mask_RCNN/issues/694">fix here </p>

  <li>coco yolov3-pytorch testing, empty tensor</li>
  <p> href="https://github.com/eriklindernoren/PyTorch-YOLOv3/commit/83c3d8b2f440f78715fb674b3d318c63ffe3eb16">fix here </p>
   
  <li>pip install, OMac SX error: command 'gcc' failed with exit status 1 </li>
  <p> export CC=clang ; export CXX=clang </p> 
  
  <li><a href="https://unix.stackexchange.com/questions/362115/how-to-keep-a-python-script-running-when-i-close-putty"> keep script running remotely when ssh connection is lost </a></li> 
  <p> nohup python train.py --batch_size 12 --n_cpu 12 >training_nohup.out& <br>
  check output by: tail -f training_nohup.out <br>
  if want to kill the process, can read the pid from nvidia-smi, or using 'ps -ef | grep python' <br>
  with the PID, do : kill -9 PID, or kill PID </p> 
  
  <li>Conda save environment to yml </li>
  <p> conda update conda <br> 
  conda env export > environment.yml <br>
  conda env export --no-builds > environment.yaml<br> <br>
  To create an enviroment from file: <br>
  firstopen environment.yml and modify the env name, save and then <br>
  conda env create -f environment.yml <br></p>

  <li>Exported the env from macOS and tried to import on Linux</li> 
  <p>Remove: <br> libcxxabi=4.0.1 <br>
  libcxx=4.0.1</p> 

  <p>Possible issue with pip: <br> pip uninstall setuptools <br> conda install -c anaconda setuptools</p> 
  
  <li><a href="https://www.linux.com/tutorials/linux-101-check-disk-space-command/"> Linux: Check Disk Space </a></li> 
  <p> df command  – Show the amount of disk space used and available on Linux file systems. 
      <br> du command – Display the amount of disk space used by the specified files and for each subdirectory. 
      <br> btrfs fi df /device/ – Show disk space usage information for a btrfs based mount point/file system. 
  </p> 

  <li> DLIB Issue #1: image not found </li> 
  <p>Solution: <br> Install from source (10 mins)<br>
  pip install 'git+https://github.com/davisking/dlib.git’</p>
  
  <li><a href="https://coderwall.com/p/ohk6cg/remote-access-to-ipython-notebooks-via-ssh/">  Remote Access to IPython Notebooks via SSH</a></li>
  <p> On the remote machine, start the IPython notebooks server: <br>  ipython notebook --no-browser --port=8889. <br> 
      <br> On the local machine, start an SSH tunnel: <br>  ssh -N -f -L localhost:8888:localhost:8889  baiqiang@turku.silo.ai . <br> 
      <br> Now open your browser on the local machine and type in the address bar: <br> localhost:8888
  </p> 
  
  <li><a href="https://github.com/mingyuliutw/UNIT/issues/56">  AttributeError: module 'tensorboard.summary._tf.summary' has no attribute 'FileWriter'</a></li>
  <p> pip install tensorflow==1.15.0 </p> 
  
  <li><a href="https://github.com/eriklindernoren/PyTorch-YOLOv3/issues/162">  OSError: image file is truncated (7 bytes not processed) #162
</a></li>
  <p> from PIL import ImageFile <br> ImageFile.LOAD_TRUNCATED_IMAGES = True </p> 
  
  <li><a href="https://git-lfs.github.com/">  git large file handling package</a></li> 
  <p> brew install git-lfs, run git lfs install </p> 
   
  <li><a href="https://answers.opencv.org/question/189349/known-issue-with-gopro-movie-codec-stops-reading-after-26-frames/">  opencv only read first few frames of gopro mp4</a></li> 
  <p> The problem is the audio track. Opencv has no decoder for audio, so need to remove audio <br>
      can do by a gross way: forget the empty frame: <br>
      <a href="https://stackoverflow.com/a/55644446">  reference </a><br>
      auto expectedFrameCount = videoCapture.get(cv::CAP_PROP_FRAME_COUNT);<br>
      for(auto frameIndex = 0; frameIndex < expectedFrameCount; frameIndex++) {<br>
        videoCapture >> frame;<br>
        if(frame.empty()) {<br>
          frameIndex--;<br>
          continue;<br>
        }<br>
        // Do useful things with the frame here...<br>
      }<br>
  </p> 
   
</ol>   
  
</body>
</html>
